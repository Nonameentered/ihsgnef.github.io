---
layout: post
title: "Deep K-Nearest Neighbors for NLP"
author: ["Eric Wallace"](https://www.ericswallace.com)
---

TLDR; Our previous [blog](https://zerobatchsize.net/2018/08/08/rawr.html) showed how interpretations of neural networks can fail due to issues in relying on the model's confidence. Here, we fix some of the confidence issues by applying [Deep k-Nearest Neighbors](https://arxiv.org/abs/1803.04765) to NLP, which improves model interpretations. 

Our [paper](blah) TODO link, full [code](https://github.com/Eric-Wallace/deep-knn), and [supplementary website](https://sites.google.com/view/
language-dknn/) provide further details. 

# Introduction

The growing ubiquity of neural networks in sensitive domains raises questions about human-perceived trust in these
machine learning systems. A critical issue is test-time interpretability, how can humans understand the reasoning behind neural network predictions? Though critical, this problem is a bit underdefined. What form should the interpretation come in? Does the model need to read out a formal statement on what caused it to predict a certain outcome? 

One common interpretation technique highlights input features based on their importance to the model's prediction. See, for example, SmoothGrad's interpretation for a model's prediction of "Obelisk".    


{: .center}
![Obelisk Photo](/images/obelisk.png)
![Obelisk Smoothgrad](/images/obelisk_smoothgrad.png)
*A gradient based interpretation generated using SmoothGrad, image taken from [PAIR site](https://pair-code.github.io/saliency/)*

For NLP, one simple method for generating interpretations is through [Leave One Out](https://arxiv.org/abs/1612.08220), remove each word one at a time from the input, and measure how much a word's removal changes the prediction confidence. If the removal considerably changes the output confidence, that word is deemed important to the prediction.

# Limitations

Recent work has identified a number of [limitations](https://arxiv.org/abs/1710.10547) for [saliency-based](https://arxiv.org/abs/1711.00867) [interpretations](https://arxiv.org/abs/1804.07781). We discussed one particular limitation in a previous [blog](https://zerobatchsize.net/2018/08/08/rawr.html), the overconfidence issue of neural models.

In short, a neural network's prediction confidence can be unreasonably high even when the input is void of any predictive information. Thus, when removing features with a method such as Leave One Out, the change in confidence may not properly reflect whether the discriminative input features have been removed. Consequently, interpretation methods that rely on confidence may fail due to this overconfidence issue in the underlying model.

One common failure mode caused by model overconfidence is the assignment of small importance values to many of the input words. This occurs because no matter which word is removed, the confidence of the model will remain high, giving each word a small, but non-zero importance value. 

![Leave One Out Saliency Map](/images/soft_attribution.png)

This is illustrated in the Figure above. Leaving out the word “diane”, causes a 0.10 drop in the probability for the positive class, whereas removing “shines” causes a smaller confidence drop of 0.09. This does not align with human perception, as “shines” is the critical word for classifying the sentiment as positive. This may indicate that the model has not learned the correct feature importance, but, these same confidence issues occur for a variety of highly predictive models (BiLSTMs, deep convolutional networks, attention networks) on diverse datasets.

# Deep k-Nearest Neighbors for NLP

Nicolas Papernot introduced the [Deep k-Nearest Algorithm](https://arxiv.org/abs/1803.04765) (henceforth DkNN). The algorithm presents a simple modification to the test-time behavior of neural models. 

In DkNN, training is conducted normally. Then, before test-time, each data point from the training set is passed through the model and the representations at each layer of the network are saved. This creates a new dataset whose features are the network’s representations and the predicted classes are the labels. To make a test prediction, the representations for each of the network’s layers are computed for the test point. Those features are then
used for k-nearest neighbors classification. This modification **does not degrade the accuracy** for image classifiers (see original work) or text classifiers (see our paper).

![DkNN Graphic](/images/panda_bus.png)

For our purposes, the main benefit of this decision making procedure is the robust measure of model uncertainty generated using the conformity score. This score is the percentage of nearest neighbors that belong to the predicted
class. For example, in the figure above, 17/18 of the nearest neighbors belong to the Panda class for the non-adversarial input, whereas 12/18 neighbors belong to the school bus for the adversarial input. This trend holds generally, on “easy” evaluation data points, the conformity score will be near 1. On out-of-domain inputs such as adversarial examples, the conformity score will be closer to zero.

Using the conformity measure, we can generate interpretations by applying a modified version of Leave One Out. After individually removing each word, rather than measuring the resulting drop in confidence, we instead measure the resulting drop in conformity. Intuitively, when we remove the "important" features, we hope the conformity score will drop significantly. Conversely, the network should be invariant to unimportant words, causing their removal to have a neglible affect on the learned representation and thus the credibility score.

# Interpretation Results

We compare out interpretation method (*Conformity* Leave One Out) to baseline methods in the Table below. The baselines are standard Leave One Out (*Confidence*), vanilla gradient-based interpretations (*Gradient*), and Leave One Out generated using a model with calibrated confidence (*Calibrated*). 

![Saliency Comparison](/images/saliency.png)

Using conformity, rather than confidence, provides a few useful benefits. First, the change in conformity better separates the importance values, dropping just 0.03 when “diane” is removed, but 0.38 when “shines” is removed. This mitigates the issue where a majority of the input words are assigned a small importance.

The second observation for confidence-based approaches is a bias towards selecting word importance based on the inherent sentiment of a word, rather than its meaning in context. For example, see “clash”, “terribly”, and “unfaithful” in the table above. The removal of these words causes a small change to the confidence. When using DKNN, the credibility
score indicates that the model’s uncertainty has not risen without these input words and thus
does not assign them any importance.

We would characterize our interpretation method as significantly higher precision, but slightly lower recall than confidence-based methods at selecting human perceived sentiment words. Conformity Leave One Out rarely assigns high importance to words that do not align with human perception of sentiment. 

# SNLI Artifacts

Our interpretation technique had higher precision at identifying the words neccesary to make a prediction. Where else can be apply this?

One area we were eager to explore further was the [annotation](https://arxiv.org/abs/1803.02324) [biases](https://arxiv.org/abs/1805.01042) identified in the SNLI dataset. Multiple groups independently showed that training models on only a portion of the input could still yield relatively high test accuracies. This occurs due to dataset artifacts that correlate with certain labels. 

We wanted to see if models that use the full input may also pick up on these biases.  We
create saliency maps using our interpretation method, using the color green to highlight
words that support the model's predicted class, and the color pink to highlight words that
support a different class. 

![SNLI Interpretations](/images/snli.png)

The first example is a randomly sampled baseline, showing how the words
"swims" and "pool" correctly support the model's prediction of contradiction.
The other examples are selected because they contain terms identified as artifacts. 
In the second example, our interpretation method assigns extremely high word importance
to "sleeping", disregarding other words necessary to predict Contradiction (i.e., the Neutral class is still possible
if "pets" is replaced with "people"). In the final two examples, the interpretation
method diagnoses the model failure, assigning high importance to "wearing", rather than focusing
positively on the shirt color. Though definitely not conclusive, this does suggest that a model
trained on the full input may pick up on these biases.
