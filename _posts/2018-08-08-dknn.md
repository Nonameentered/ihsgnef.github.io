---
layout: post
title: "Deep K-Nearest Neighbors for NLP"
author: "Eric Wallace"
---

TLDR; Our previous [blog](https://zerobatchsize.net/2018/08/08/rawr.html) showed how interpretations can fail due to issues in relying on model confidence. Here, we fix some of the confidence issues by applying [Deep k-Nearest Neighbors](https://arxiv.org/abs/1803.04765) to NLP, which improves model interpretations. 

Our [paper](blah) TODO link, full [code](https://github.com/Eric-Wallace/deep-knn), and [supplementary website](https://sites.google.com/view/
language-dknn/) provide further details. 

# Introduction


# Limitations


# Deep k-Nearest Neighbors for NLP


# Interpretation Results


# SNLI Artifacts

Our interpretation technique had higher precision at identifying the words neccesary to make a prediction. Where else can be apply this?

One area we were eager to explore further was the [annotation](https://arxiv.org/abs/1803.02324) [biases](https://arxiv.org/abs/1805.01042) identified in the SNLI dataset. Multiple groups independently showed that training models on only a portion of the input could still yield relatively high test accuracies. This occurs due to dataset artifacts that correlate with certain labels. 

We wanted to see if models that use the full input may also pick up on these biases.  We
create saliency maps using our interpretation method, using the color green to highlight
words that support the model's predicted class, and the color pink to highlight words that
support a different class. 

![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

The first example is a randomly sampled baseline, showing how the words
"swims" and "pool" correctly support the model's prediction of contradiction.
The other examples are selected because they contain terms identified as artifacts. 
In the second example, our interpretation method assigns extremely high word importance
to "sleeping", disregarding other words necessary to predict Contradiction (i.e., the Neutral class is still possible
if "pets" is replaced with "people"). In the final two examples, the interpretation
method diagnoses the model failure, assigning high importance to "wearing", rather than focusing
positively on the shirt color. Though definitely not conclusive, this does suggest that a model
trained on the full input may pick up on these biases.
